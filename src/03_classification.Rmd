---
title: "Metabarcoding Classification"
description: |
  A QIIME2-based metabarcoding workflow. Part 3.
author:
  - name: Your Name Here
    url: https://example.com/
    affiliation: Example Organisation
    affiliation_url: https://example.com/
    orcid_id: 0000-0000-0000-0000
  - name: Jessica Chung
    url: https://github.com/jessicachung
    affiliation: Melbourne Bioinformatics, PEARG
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 4
    df_print: paged
    code_folding: true
    highlight: haddock
    highlight_downlit: false
    css: "custom.css"
---


This notebook performs classification for your metabarcoding analysis. This notebook depends on the outputs of the previous notebooks, `01_preprocessing.Rmd` where we preprocessed the sequencing data, and `02_quantification` where we quantified ASVs.


```{r}
# TODO: customise slurm n_threads if using slurm or not
```


```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=TRUE, message=TRUE, error=TRUE, echo=TRUE, results="hold")
knitr::opts_knit$set(root.dir = "..")
options(digits=4)
options(width=120)
```


# Load

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(systemPipeR)
  library(tidyverse)
  library(here)
  library(patchwork)
  library(rmarkdown)
  library(fs)
  library(qiime2R)
})

xaringanExtra::use_panelset()

# Load workspace from the previous notebook
load(here::here("cache/02.RData"))
```

```{r}
# Append qiime2 path to PATH
current_path_env <- Sys.getenv("PATH")

# Also set qiime2 conda environment in batchtools.slurm.tmpl
Sys.setenv(PATH=paste0("/mnt/galaxy/gvl/software/shared_envs/qiime2_2023-07/bin:",
                       current_path_env))
Sys.setenv(PYTHONPATH="/mnt/galaxy/gvl/software/shared_envs/qiime2_2023-07/lib/python3.8/site-packages/")
```

```{r}
# For the mozzie server, the root disk can run out of space, so it's safer to set the tmp directory 
# to a different location
tmp_dir_path <- "/mnt/galaxy/tmp"
# tmp_dir_path <- "/tmp"
```

```{r}
use_slurm <- TRUE
slurm_threads <- 8
slurm_resources <- list(partition="main", ntasks=1, ncpus=slurm_threads, memory=4096)
slurm_n_jobs <- 10
```

-----

# Create databases for classification

```{r}
ref_db_dir <- "/mnt/galaxy/home/mecarew/Documents/DNA_reference_db_construction/refdb/final_library"

db_coi_ids=list(
  short=file.path(ref_db_dir, "short_db_2023-07.txt"),
  left=file.path(ref_db_dir, "left_db_2023-07.txt"),
  right=file.path(ref_db_dir, "right_db_2023-07.txt"),
  long=file.path(ref_db_dir, "long_db_2023-07.txt")
)

# FASTA file with COI sequences
db_fasta_files=list(
  short=file.path(ref_db_dir, "short_db_2023-07.fasta"),
  left=file.path(ref_db_dir, "left_db_2023-07.fasta"),
  right=file.path(ref_db_dir, "right_db_2023-07.fasta"),
  long=file.path(ref_db_dir, "long_db_2023-07.fasta")
)
```



```{r}
taxonomy_ref <- data.frame(SampleName=names(db_fasta_files),
                           db_coi_id_file=unlist(db_coi_ids[names(db_fasta_files)]),
                           db_fasta_file=unlist(db_fasta_files[names(db_fasta_files)]))
```


```{r}
classification_dir <- file.path(results_dir, "classification")
mkdir(classification_dir)
classification_db_dir <- file.path(results_dir, "classification/db")
mkdir(classification_db_dir)

# Setup targets
targets <- data.frame(FileName=taxonomy_ref$db_fasta_file,
                      taxonomy_id_file=taxonomy_ref$db_coi_id_file,
                      SampleName=taxonomy_ref$SampleName,
                      amplicon_group=taxonomy_ref$SampleName,
                      output_prefix=paste0(taxonomy_ref$SampleName))

targets <- targets %>% 
  mutate(sequences_output=
           file.path(classification_db_dir, paste0(amplicon_group, "_feature_sequences.qza")),
         taxonomy_output=
           file.path(classification_db_dir, paste0(amplicon_group, "_feature_taxonomy.qza"))
)

database_targets <- here("tmp/database_targets.txt")

# WIP: For the moment, only run the short/right amplicions
write_targets(targets %>% filter(SampleName %in% c("short", "right")), file=database_targets)
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
create_database <- loadWorkflow(targets=database_targets,
                         wf_file="qiime_database.cwl", 
                         input_file="qiime_database.yml", dir_path=cwl_path)
create_database <- set_logs_dir(create_database, logs_dir)
create_database@yamlinput$database_wrapper$path <- 
  file.path(cwl_path, "qiime_database_wrapper.sh")
create_database@yamlinput$output_dir$path <- classification_db_dir

create_database <- renderWF(create_database, 
                          inputvars=c(FileName="_INPUT_FASTA_",
                                      taxonomy_id_file="_INPUT_TAXONOMY_",
                                      output_prefix="_OUTPUT_PREFIX_",
                                      sequences_output="_OUTPUT_FASTA_DB_"))
run_jobs(create_database)
```


:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(create_database)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(classification_db_dir)
```

:::

::::

-----

# Assign taxonomy


## Preliminary search

Run vsearch on sequences to get the % identity top hit for each ASV. We do this because the step after uses the top % identity to determine the threshold of other matches that should be considered when classifying the ASV.

The newest version of `classify-consensus-vsearch` has an `--o-search-results` option, so we no longer need to run vsearch manually.


```{r}
classification_prelim_dir <- file.path(results_dir, "classification/preliminary")
mkdir(classification_prelim_dir)

# Setup targets
targets <- data.frame(FileName=extract_targets(dada2, "seq_output"),
                      SampleName=extract_targets(dada2, "SampleName"),
                      amplicon_group=extract_targets(dada2, "amplicon_group"))

# Join with database paths
ref_db <- data.frame(
  amplicon_group=extract_targets(create_database, "amplicon_group"),
  sequences_db=extract_targets(create_database, "sequences_output"),
  taxonomy_db=extract_targets(create_database, "taxonomy_output")
)
targets <- left_join(targets, ref_db, by="amplicon_group")
stopifnot(all(! is.na(targets)))

# This can be changed if you want different parameters
targets$min_consensus <- 0.51
targets$p_identity <- 0.8

targets <- targets %>% 
  mutate(classification_output=
           file.path(classification_prelim_dir, paste0(amplicon_group, "_prelim_classification.qza")),
         search_results_output=
           file.path(classification_prelim_dir, paste0(amplicon_group, "_prelim_search_results.qza"))
  )

prelim_classification_targets <- here("tmp/classification_prelim_targets.txt")

# WIP: For the moment, only run the short and right amplicions
write_targets(targets %>% filter(amplicon_group %in% c("short", "right")), file=prelim_classification_targets)
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}


```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
prelim_classification <- loadWorkflow(targets=prelim_classification_targets,
                                           wf_file="qiime_prelim_classification.cwl", 
                                           input_file="qiime_prelim_classification.yml", dir_path=cwl_path)
prelim_classification <- set_logs_dir(prelim_classification, logs_dir)
prelim_classification@yamlinput$taxonomy_wrapper$path <- 
  file.path(cwl_path, "qiime_classification_wrapper.sh")
prelim_classification@yamlinput$tmp_dir <- tmp_dir_path
prelim_classification@yamlinput$n_threads <- slurm_threads
prelim_classification@yamlinput$output_dir$path <- classification_prelim_dir
prelim_classification <- renderWF(prelim_classification, 
                                       inputvars=c(FileName="_INPUT_REP_SEQ_",
                                                   sequences_db="_INPUT_SEQ_DB_",
                                                   taxonomy_db="_INPUT_TAXONOMY_DB_",
                                                   amplicon_group="_AMPLICON_GROUP_",
                                                   p_identity="_PERC_IDENTITY_",
                                                   min_consensus="_MIN_CONSENSUS_",
                                                   classification_output="_CLASSIFICATION_OUTPUT_",
                                                   search_results_output="_SEARCH_OUTPUT_"))

run_jobs(prelim_classification)
```


:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(prelim_classification)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(classification_dir)
```

:::

::::




## Process preliminary results

Process the search results to get % identity match thresholds. Currently we use the top % identity match minus 3 for the threshold of matches to consider, but this can be changed using the `offset` parameter in the `get_thresholds` function.

```{r}
classification_thresholds_dir <- file.path(results_dir, "classification/thresholds")
mkdir(classification_thresholds_dir)
```

```{r}
get_thresholds <- function(qza_path, offset=3, max_threshold=80) {
  results <- read_qza(qza_path)
  thresholds <- results$data %>% 
    group_by(V1) %>% 
    summarise(max_p_identity=max(V3)) %>%
    mutate(threshold=max_p_identity - offset) %>%
    mutate(threshold=ifelse(threshold < max_threshold, max_threshold, threshold)) %>%
    rename(`feature-id`=V1)
  return(thresholds)
}
```

```{r}
write_id_files_for_thresholds <- function(threshold_df, file_path, file_prefix) {
  unique_thresholds <- sort(unique(threshold_df$threshold))
  for (t in unique_thresholds) {
    output_path <- sprintf("%s/%s_%.03f.txt", file_path, file_prefix, t/100)
    write.table(threshold_df %>% filter(threshold == t) %>% select(`feature-id`), 
                file=output_path, quote=FALSE, row.names=FALSE)
  }
  file_list <- data.frame(filename=sprintf("%s/%s_%.03f.txt", file_path, file_prefix, unique_thresholds/100),
                          threshold=unique_thresholds/100)
  write.table(file_list, 
              file=sprintf("%s/%s_files.txt", file_path, file_prefix), 
              quote=FALSE, row.names=FALSE, col.names=FALSE, sep="\t")
}
```


```{r}
thresholds_list <- list()
for (amplicon_group in amplicon_groups) {
  # WIP: only short and right amplicon groups for now
  if (! amplicon_group %in% c("short", "right")) {
    next
  }
  print(amplicon_group)
  prelim_search_filename <- sprintf("%s/%s_prelim_search_results.qza", classification_prelim_dir, amplicon_group)
  thresholds_list[[amplicon_group]] <- get_thresholds(prelim_search_filename)
  write_id_files_for_thresholds(thresholds_list[[amplicon_group]], 
                                file_path=classification_thresholds_dir,
                                file_prefix=amplicon_group)
}
```


```{r}
threshold_file_list <- data.frame(
  amplicon_group=c("short", "right"),
  file_list=c(here("results/classification/thresholds/ncbi_short_files.txt"),
              here("results/classification/thresholds/ncbi_short_files.txt"))
)
paged_table(threshold_file_list)
```


## Classification

Classify ASVs based on the percentage identity threshold from the previous preliminary search step.

```{r}
# Setup targets
targets <- data.frame(FileName=extract_targets(dada2, "seq_output"),
                      SampleName=extract_targets(dada2, "SampleName"),
                      amplicon_group=extract_targets(dada2, "amplicon_group"))

# Join with NCBI database paths
ref_db <- data.frame(
  amplicon_group=extract_targets(create_database, "amplicon_group"),
  sequences_db=extract_targets(create_database, "sequences_output"),
  taxonomy_db=extract_targets(create_database, "taxonomy_output")
)
targets <- left_join(targets, ref_db, by="amplicon_group")
stopifnot(all(! is.na(targets)))

# Join with ID file lists
targets <- left_join(targets, threshold_file_list, by="amplicon_group")
targets <- targets %>% filter(! is.na(file_list))

# This can be changed if you want different parameters
targets$min_consensus <- 0.51

targets <- targets %>% 
  mutate(classification_output=
           file.path(classification_dir, paste0(amplicon_group, "_classification.qza"))
  )

classification_targets <- here("tmp/classification_targets.txt")

# WIP: For the moment, only run the short and right amplicions
write_targets(targets %>% filter(amplicon_group %in% c("short", "right")), file=classification_targets)
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
classification <- loadWorkflow(targets=classification_targets,
                                           wf_file="qiime_classification_by_threshold.cwl", 
                                           input_file="qiime_classification_by_threshold.yml", dir_path=cwl_path)
classification <- set_logs_dir(classification, logs_dir)
classification@yamlinput$taxonomy_wrapper$path <- 
  file.path(cwl_path, "qiime_classification_by_threshold_wrapper.sh")
classification@yamlinput$tmp_dir <- tmp_dir_path
classification@yamlinput$n_threads <- slurm_threads
classification@yamlinput$output_dir$path <- classification_dir
classification <- renderWF(classification, 
                           inputvars=c(FileName="_INPUT_REP_SEQ_",
                                       sequences_db="_INPUT_SEQ_DB_",
                                       taxonomy_db="_INPUT_TAXONOMY_DB_",
                                       min_consensus="_MIN_CONSENSUS_",
                                       classification_output="_CLASSIFICATION_OUTPUT_",
                                       file_list="_FILE_LIST_"))

run_jobs(classification)
```


:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(classification)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(classification_dir)
```

:::

::::


-----

# Save output

Save output so it can be loaded in subsequent notebooks.

```{r}
save.image(here("cache/03.RData"))
```


-----

# Session Info

```{r}
if (nzchar(system.file(package="devtools"))) {
  devtools::session_info()
} else {
  sessionInfo()
}
```




```{r}
knitr::knit_exit()
```




# =-=-=-=-=-=-=-=-=-=

# WIP


```{r}
short_lulu <- readRDS(here("results/lulu/short_lulu.rds"))
short_classifications <- read_qza(here("results/classification/short_classification.qza"))
short_seq <- read_qza(here("results/dada2/short_representative_sequences.qza"))
```

```{r}
short_classifications$data %>% head(100)
```

```{r}
short_lulu$curated_table
```


```{r}
short_df <- merge(short_classifications$data, thresholds_list[["short"]], by.x="Feature.ID", by.y="feature-id")
```

```{r}
short_seq$data %>% head
```


```{r}
short_df %>% arrange(desc(max_p_identity)) %>% head(100)
```

```{r}
table(short_df$Taxon == "Unassigned")
```


```{r}
short_df %>% filter(Taxon == "Unassigned") %>% pull(max_p_identity) %>% table
```




```{r}
short_search_results <- read_qza(here("results/classification/preliminary/short_prelim_search_results.qza"))
```

```{r}
short_search_results$data %>% dim
```


```{r}
# ncbi_cois <- read.table(db_coi_ids[["short"]], sep="\t")
# colnames(ncbi_cois) <- c("id", "taxonomy")
```


```{r}
# short_hits <- merge(short_search_results$data, ncbi_cois, by.x="V2", by.y="id", all.x=TRUE)
```



# Filtering ASVs

This can be changed with future feedback.  

We could use https://docs.qiime2.org/2023.5/plugins/available/feature-table/filter-features-conditionally/
if we want simple filtering, but if we want to do something complex, better to do it manually.


```{r}
all_tables <- list(
  short=short_lulu$curated_table,
  right=right_lulu$curated_table
)
```


```{r}
# Get sample names and groups
sample_groups <- samples_df$factor
names(sample_groups) <- samples_df$sample_name
sample_groups
```

```{r}
# Need to rename columns to get rid of the prefix X character if sample names start with a digit
if (all(! colnames(all_tables[[1]]) %in% names(sample_groups))) {
  all_tables <- lapply(all_tables, function(x) {
    colnames(x) <- str_remove(colnames(x), "^X")
    return(x)
  })
}
```



```{r}
# Filter the feature tables. Expect some false positives due to mis-tagging.
# If an observation is only seen in one replicate and is very low compared to other samples,
# consider it a false positive and replace the number with a zero

# Filter 1
# If the number of barcodes for a sample is only seen in one replicate and the number is below
# <threshold_absolute> observations, then remove it
threshold_absolute <- 0

# Filter 2
# Using the samples that have the largest observation of the barcode, take the average number
# from all its replicates. If the barcode is only seen in one replicate of another sample,
# if the number of barcodes is less than <threshold_relative_to_max_sample> * max_average, 
# remove it
# e.g. For barcode 1, if sample A has 3 replicates with (0, 0, 4) observations, and sample B has 3
# observations (500, 550, 450). The average of sample B is 500. If <threshold_relative_to_max_sample>
# is set to 0.01, the threshold is 5. Since sample A's single observation is less than 5, the
# observation will be removed
threshold_relative_to_max_sample <- 0.01
```

Currently applying filter 1, then filter 2 on the results after filter 1. But can be changed to do both filters independently.

```{r}
f1_filtered <- list()
for (l in names(all_tables)) {
  matrix_list <- list()
  for (g in unique(sample_groups)) {
    sample_ids <- names(sample_groups)[sample_groups == g]
    x <- all_tables[[l]][,sample_ids]
    ok1 <- x > 0
    ok2 <- x >= threshold_absolute
    # Keep ASVs if they have observations in at least two samples within a replicate group or have 
    # observations > <threshold_absolute> in one sample. Else, set observations to zero for that ASV
    # for that group
    ok_asvs <- rowSums(ok1) > 1 | rowSums(ok2) >= 1
    # Check removed ASVs that are non-zero
    # print(x[! ok_asvs,][rowSums(x[! ok_asvs,]) > 0,])
    # Set failed ASVs to zero
    x[! ok_asvs,] <- 0
    matrix_list[[g]] <- x
  }
  names(matrix_list) <- NULL
  f1_filtered[[l]] <- do.call(cbind, matrix_list)
}


# Check dimensions
sapply(f1_filtered, dim)

# Check dimensions of non-zero ASVs
sapply(f1_filtered, function(x) {x[rowSums(x) > 0,] %>% dim})
```



```{r}
f2_filtered <- list()
for (l in names(all_tables)) {
  matrix_list <- list()
  # First get mean observations for each sample group
  mean_list <- list()
  for (g in unique(sample_groups)) {
    sample_ids <- names(sample_groups)[sample_groups == g]
    x <- f1_filtered[[l]][,sample_ids]
    mean_list[[g]] <- rowMeans(f1_filtered[[l]][,sample_ids])
  }
  
  # Get the max mean observation for each ASV
  max_mean_obs <- do.call(cbind, mean_list) %>% apply(1, max)
  
  # Get the threshold
  f2_threshold <- max_mean_obs * threshold_relative_to_max_sample
  
  # If there's only a single observation in the sample group and that observation is 
  # less than <f2_threshold>, set observations to zero for that ASV in the group
  for (g in unique(sample_groups)) {
    sample_ids <- names(sample_groups)[sample_groups == g]
    x <- f1_filtered[[l]][,sample_ids]
    ok1 <- x > 0
    ok2 <- x >= f2_threshold
    ok_asvs <- rowSums(ok1) > 1 | rowSums(ok2) >= 1
    # Check removed ASVs that are non-zero
    # print(x[! ok_asvs,][rowSums(x[! ok_asvs,]) > 0,])
    # Set failed ASVs to zero
    x[! ok_asvs,] <- 0
    matrix_list[[g]] <- x
  }
  names(matrix_list) <- NULL
  f2_filtered[[l]] <- do.call(cbind, matrix_list)
}

# Check dimensions
sapply(f2_filtered, dim)

# Check dimensions of non-zero ASVs
sapply(f2_filtered, function(x) {x[rowSums(x) > 0,] %>% dim})
```






```{r}
# Sanity check ASVs that changed
diff <- lapply(names(all_tables), function(l) {
  diff <- rowMeans(all_tables[[l]] == f2_filtered[[l]]) != 1
  names(diff)[diff]
})

# e.g.
all_tables[[1]][diff[[1]],]
f2_filtered[[1]][diff[[1]],]
```



# Truncate Taxonomy?

```
Generally, any DNA barcode sequence match to the above 97% would be considered a species level match. At present, I truncate matches of >96%->95% to genus, <95% to >92% to family
```




