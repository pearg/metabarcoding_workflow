---
title: "Metabarcoding Preprocessing"
description: |
  A QIIME2-based metabarcoding workflow. Part 1.
author:
  - name: Your Name Here
    url: https://example.com/
    affiliation: Example Organisation
    affiliation_url: https://example.com/
    orcid_id: 0000-0000-0000-0000
  - name: Jessica Chung
    url: https://github.com/jessicachung
    affiliation: Melbourne Bioinformatics, PEARG
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 4
    df_print: paged
    code_folding: true
    highlight: haddock
    highlight_downlit: false
    css: "custom.css"
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=TRUE, message=TRUE, error=TRUE, echo=TRUE, results="hold")
knitr::opts_knit$set(root.dir = "..")
options(digits=4)
options(width=120)
```



This notebook runs through some preprocessing steps for metabarcoding analysis. First we'll load in some sample information containing sample metadata and primer combinations, then we'll import the FASTQ sequencing files into QIIME artifact files. We'll then demultiplex sequences based on primer combinations and split the sequences into amplicon groups. Lastly, we'll trim adapters and calculate some summary statistics.

# User settings

The first thing we'll do is set the directory that stores the workflow outputs.

```{r}
# You can use the here::here function to define a path relative to the project 
#  directory or use an absolute path
results_dir <- here::here("results")

# Log files will be stored inside the logs directory inside the results directory
logs_dir <- file.path(results_dir, "logs")
```

The outputs of this workflow will be stored in:  
**`r results_dir`**.

We also have the option to run the jobs in this workflow using SLURM if you're working on a cluster
that uses it.

```{r class.source='fold-show'}
# use_slurm <- FALSE
use_slurm <- TRUE

# Settings for slurm (you can ignore these if `use_slurm` is set to FALSE)
slurm_conffile <- here::here("src/.batchtools.conf.R")
slurm_template <- here::here("src/batchtools.slurm.tmpl")
slurm_resources <- list(partition="main", ntasks=1, ncpus=1, memory=4096)
slurm_n_jobs <- 10
```


# Load libraries and files

We begin by loading the necessary libraries and files.

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(systemPipeR)
  library(tidyverse)
  library(here)
  library(patchwork)
  library(rmarkdown)
  library(fs)
})

xaringanExtra::use_panelset()
```

And then we'll add QIIME2 to the environment path.

```{r}
# Append qiime2 path to PATH
current_path_env <- Sys.getenv("PATH")

# Also set qiime2 conda environment in batchtools.slurm.tmpl
Sys.setenv(PATH=paste0("/mnt/galaxy/gvl/software/shared_envs/qiime2_2023-07/bin:",
                       current_path_env))
Sys.setenv(PYTHONPATH="/mnt/galaxy/gvl/software/shared_envs/qiime2_2023-07/lib/python3.8/site-packages/")
```

And define some functions (might move this elsewhere...)

```{r}
extract_targets <- function(x, name) {
  sapply(targets(x), function(x) x[[name]])
}

write_targets <- function(x, file) {
  write.table(x, file=file, quote=FALSE, sep="\t", row.names=FALSE)
}

mkdir <- function(x, quiet=TRUE) {
  if (dir.exists(x)) {
    if (! quiet) message("Directory ", x, " already exists")
  } else {
    dir.create(x)
    if (! quiet) message("Directory ", x, " created")
  }
}

set_logs_dir <- function(wf, logs_dir) {
  wf@yamlinput$results_path$path <- logs_dir
  return(wf)
}

run_jobs <- function(wf, slurm=use_slurm, resources=slurm_resources, quiet=TRUE) {
  if (all(file_exists(unlist(output(wf))))) {
    if (! quiet) message("Output files already exist.")
    runCommandline(wf)
  } else if (! slurm) {
    if (! quiet) message("Running jobs on a single machine")
    runCommandline(wf)
  } else {
    if (! quiet) message("Running jobs on a cluster using SLURM")
    reg <- clusterRun(wf, conffile=slurm_conffile, template=slurm_template, resourceList=resources,
                      Njobs=slurm_n_jobs)
    check <- batchtools::getStatus(reg=reg)
    print(check)
    while(! ((check$done + check$error == check$defined) & (check$queued == 0) & 
          (check$running == 0)) ) {
      Sys.sleep(5)
      check <- batchtools::getStatus(reg=reg)
    }
    if (! quiet) message("Finished!")
    print(check)
    return(reg)
  }
}
```


### Sample information

A file containing sample information is required. We'll read in `data/metadata/samples.tsv` which contains one line for each sample. The columns in the TSV file are required to be:

- `filename_r1`: the path to the FASTQ file containing forward reads of the sequencing run 
- `filename_r2`: the path to the FASTQ file containing reverse reads of the sequencing run
- `sample_name`: a unique sample name
- `factor`: a factor of interest for your dataset (if you don't have a factor of interest, you can set the values as the same as the sample name)

```{r}
samples_df <- read.table(here("data/metadata/samples.tsv"), header=TRUE, stringsAsFactors=FALSE)
paged_table(samples_df)
```



### Primers

We also need a file that contains the primers that the metabarcoding project uses. We'll read in `ref/primers.tsv` which contains one row for each primer used. The columns of this file are required to be:

- `primer`: the unique primer name
- `primer_seq`: the primer sequence
- `primer_rc`: the reverse complement of the primer sequence.

```{r}
primers_filename <- here("ref/primers.tsv")
suppressMessages(
  primers_df <- read_tsv(primers_filename, col_names=c("primer", "primer_seq", "primer_rc"))
)
paged_table(primers_df)
```



### Primer combinations

Lastly, we need the group that each primer combination corresponds to. This data is read in from `ref/metadata.tsv` and contains one row for each primer combination. The columns of this file are required to be:

- `sample-id`: the unique name for the primer combination
- `group`: the metabarcoding group that the combination belongs to
- `forward-primer`: the name of the forward primer
- `forward-sequence`: the sequence corresponding to the forward primer
- `reverse-primer`: the name of the reverse primer
- `reverse-sequence`: the sequence corresponding to the reverse primer

```{r}
metadata_filename <- here("ref/metadata.tsv")
suppressMessages(
  metadata_df <- read_tsv(metadata_filename)
)
paged_table(metadata_df)
```


-----

# Import data

To import data into QIIME, the FASTQ files for each library need to be in separate directories.
First, let's check that all files exist from the `samples.tsv` file provided.

```{r}
# Check if all files exist from samples_df
data.frame(sample_name=samples_df$sample_name,
           r1_exists=file.exists(here(samples_df$filename_r1)),
           r2_exists=file.exists(here(samples_df$filename_r2))) %>%
  paged_table()
```

If all the files exist, we can now create a separate directory for each sample and link back to the original FASTQ files using symbolic links (shortcuts).

```{r}
# Create some directories first
dir_create(results_dir)
dir_create(logs_dir)
dir_create(here("tmp"))

# Create symlinks
import_data_dir <- file.path(results_dir, "import_data")
import_targets <- list()
for (i in seq_len(nrow(samples_df))) {
  x <- as.list(samples_df[i,])
  lib_dir <- file.path(import_data_dir, x$sample_name)
  import_targets[[i]] <- lib_dir
  if (! dir.exists(lib_dir)) {
    dir.create(lib_dir, recursive=TRUE)
  }
  stopifnot(str_detect(x$filename_r1, "R1"))
  stopifnot(str_detect(x$filename_r2, "R2"))
  if (! file.exists(file.path(lib_dir, "forward.fastq.gz"))) {
    file.symlink(from=file.path(here(x$filename_r1)),
               to=file.path(lib_dir, "forward.fastq.gz"))
  }
  if (! file.exists(file.path(lib_dir, "reverse.fastq.gz"))) {
    file.symlink(from=file.path(here(x$filename_r2)),
                 to=file.path(lib_dir, "reverse.fastq.gz"))
  }
}
import_targets <- unlist(import_targets)
```

Let's check if these files have been created using the `dir_tree` function:

```{r}
# If there are already qza files generated, don't list them here
dir_tree(here(import_data_dir), regexp="*qza", invert=TRUE)
```

Now we can import these files into a QIIME artifact file.

```{r}
targets <- data.frame(FileName=import_targets,
                      SampleName=basename(import_targets),
                      Factor=basename(import_targets),
                      output=paste0(import_targets, ".qza"))
import_targets <- here("tmp/import_targets.txt")
write.table(targets, file=import_targets, sep="\t", row.names=FALSE, quote=FALSE)
cwl_path <- here("param/cwl")
```

And run the command to import files into QIIME2 artifact files.

::::: {.panelset}

::: {.panel}

#### Run commands {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
import <- loadWorkflow(targets=import_targets, wf_file="qiime_import.cwl", 
    input_file="qiime_import.yml", dir_path=cwl_path)
import <- renderWF(import, inputvars=c(FileName="_INPUT_PATH_", output="_OUTPUT_PATH_"))
import <- set_logs_dir(import, logs_dir)
run_jobs(import)
```

:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# If you want to check the commands run, you can use `cmdlist` to do so
cmdlist(import)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(import_data_dir)
```

:::

::::

-----

# Demux

Demuxing is the step that separates primer combinations within the sample files into separate files.

```{r}
# Setup targets for demux step
demux_dir <- file.path(results_dir, "demux")
mkdir(demux_dir)
targets <- data.frame(FileName=extract_targets(import, "output"),
                      SampleName=extract_targets(import, "SampleName"),
                      Factor=extract_targets(import, "Factor"),
                      metadata_path=metadata_filename) %>%
  mutate(demux_output=file.path(demux_dir, paste0(SampleName, "_demux.qza")),
         discard_output=file.path(demux_dir, paste0(SampleName, "_discard.qza")),
         log_output=file.path(demux_dir, paste0(SampleName, "_cutadapt.log")))
demux_targets <- here("tmp/demux_targets.txt")
write_targets(targets, file=demux_targets)
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
demux <- loadWorkflow(targets=demux_targets, wf_file="qiime_demux.cwl", 
    input_file="qiime_demux.yml", dir_path=cwl_path)
demux <- set_logs_dir(demux, logs_dir)
demux <- renderWF(demux, inputvars=c(FileName="_INPUT_PATH_", metadata_path="_PRIMERS_FILE_",
                                     demux_output="_OUTPUT_SEQUENCES_", discard_output="_OUTPUT_UNTRIMMED_",
                                     log_output="_LOG_PATH_"))
run_jobs(demux)
```

:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(demux)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(demux_dir)
```

:::

::::

-----

# Split amplicon groups

Split QIIME qza files into separate files for each amplicon group.

```{r}
# Setup targets
targets <- data.frame(FileName=extract_targets(demux, "demux_output"),
                      SampleName=extract_targets(demux, "SampleName"),
                      Factor=extract_targets(demux, "Factor"),
                      metadata_path=metadata_filename)

target_list <- list()
for (amplicon_group in unique(metadata_df$group)) {
  target_list[[amplicon_group]] <- targets %>%
    mutate(condition=sprintf("[group] == '%s'", amplicon_group),
           output=file.path(demux_dir, paste0(SampleName, "_demux_", amplicon_group, ".qza")),
           sample_id=SampleName,
           SampleName=paste(SampleName, amplicon_group, sep="_"),
           amplicon_group=amplicon_group)
}

targets <- do.call(rbind, target_list)
split_targets <- here("tmp/split_targets.txt")
write_targets(targets, file=split_targets)
```

::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
split <- loadWorkflow(targets=split_targets, wf_file="qiime_split.cwl", 
    input_file="qiime_split.yml", dir_path=cwl_path)
split <- set_logs_dir(split, logs_dir)
split <- renderWF(split, inputvars=c(FileName="_INPUT_PATH_", metadata_path="_METADATA_PATH_",
                                     condition="_CONDITION_", output="_OUTPUT_PATH_"))
run_jobs(split)
```

:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(split)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(demux_dir)
```

:::

::::


-----

# Trim adapters

Use reverse complement sequences to trim any 3' adapters from reads from short fragments.

```{r}
trimmed_dir <- file.path(results_dir, "trimmed")
mkdir(trimmed_dir)
```

Extract adapter sequences from the metadata info.

```{r}
amplicon_groups <- unique(metadata_df$group)
adapter_seq <- list()
for (p in amplicon_groups) {
  primers_f <- metadata_df %>% filter(group == p) %>% pull(`forward-primer`)
  primers_r <- metadata_df %>% filter(group == p) %>% pull(`reverse-primer`)
  adapter_f <- primers_df %>% filter(primer %in% primers_f) %>% pull(primer_rc)
  adapter_r <- primers_df %>% filter(primer %in% primers_r) %>% pull(primer_rc)
  adapter_seq[[p]][["f"]] <- adapter_f
  adapter_seq[[p]][["r"]] <- adapter_r
}
```

```{r}
# Setup targets
targets <- data.frame(FileName=extract_targets(split, "output"),
                      SampleName=extract_targets(split, "SampleName"),
                      Factor=extract_targets(split, "Factor"),
                      metadata_path=metadata_filename,
                      sample_id=extract_targets(split, "sample_id"),
                      amplicon_group=extract_targets(split, "amplicon_group"))

targets <- targets %>% 
  mutate(adapter_f=sapply(targets$amplicon_group, function(x) paste(adapter_seq[[x]][["f"]], collapse=" ")),
         adapter_r=sapply(targets$amplicon_group, function(x) paste(adapter_seq[[x]][["r"]], collapse=" ")),
         output=file.path(trimmed_dir, paste0(SampleName, ".qza")),
         log_path=file.path(trimmed_dir, paste0(SampleName, "_cutadapt_trim.log")))

trimmed_targets <- here("tmp/trimmed_targets.txt")
write_targets(targets, file=trimmed_targets)
```

::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
trim <- loadWorkflow(targets=trimmed_targets, wf_file="qiime_trim.cwl", 
    input_file="qiime_trim.yml", dir_path=cwl_path)
trim <- set_logs_dir(trim, logs_dir)
trim <- renderWF(trim, inputvars=c(FileName="_INPUT_PATH_", adapter_f="_ADAPTER_F_",
                                   adapter_r="_ADAPTER_R_", output="_OUTPUT_PATH_",
                                   log_path="_LOG_PATH_"))
run_jobs(trim)
```

:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(trim)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(trimmed_dir)
```

:::

::::

-----

# Trimming stats

The code to extract the trimming stats from cutadapt's log files is a bit ad hoc and will likely break if cutadapt changes it's verbose output. If this breaks, skip or comment out the code in this section, since nothing downstream depends on it.

You can also view the log files in more detail in:  
**`r trimmed_dir`**.

```{r}
extract_cutadapt_stats <- function(summary_text) {
  summary <- list()
  i <- grep("Total read pairs processed", summary_text)
  stopifnot(length(i) == 1)
  summary[["input_read_pairs"]] <- str_remove_all(str_extract(summary_text[i], "[\\d,]+$"), ",")
  i <- grep("Read 1 with adapter:", summary_text)
  stopifnot(length(i) == 1)
  summary[["r1_detect_adapter"]] <- str_remove_all(str_match(summary_text[i], "([\\d,]+) \\([\\d.%]+\\)$")[,2], ",")
  i <- grep("Read 2 with adapter:", summary_text)
  stopifnot(length(i) == 1)
  summary[["r2_detect_adapter"]] <- str_remove_all(str_match(summary_text[i], "([\\d,]+) \\([\\d.%]+\\)$")[,2], ",")
  i <- grep("Pairs that were too short:", summary_text)
  stopifnot(length(i) == 1)
  summary[["filtered_pairs"]] <- str_remove_all(str_match(summary_text[i], "([\\d,]+) \\([\\d.%]+\\)$")[,2], ",")
  i <- grep("Pairs written \\(passing filters\\):", summary_text)
  stopifnot(length(i) == 1)
  summary[["passed_pairs"]] <- str_remove_all(str_match(summary_text[i], "([\\d,]+) \\([\\d.%]+\\)$")[,2], ",")
  i <- grep("Total basepairs processed:", summary_text)
  stopifnot(length(i) == 1)
  summary[["total_input_bp"]] <- str_remove_all(str_match(summary_text[i], "([\\d,]+) bp$")[,2], ",")
  i <- grep("Total written \\(filtered\\):", summary_text)
  stopifnot(length(i) == 1)
  summary[["total_output_bp"]] <- str_remove_all(str_match(summary_text[i], "([\\d,]+) bp \\([\\d.%]+\\)$")[,2], ",")
  return(sapply(summary, as.numeric))
}
```


```{r}
indiv_trim_stats <- list()
for (i in seq_len(nrow(targets))) {
  log_file_txt <- readLines(targets$log_path[i])
  sample_name <- targets$SampleName[i]
  # Each individual primer combination has its own stats
  summary_indicies <- which(log_file_txt == "=== Summary ===")
  if (length(summary_indicies) == 0) {
    indiv_trim_stats[[sample_name]] <- matrix(0, nrow=7)
    rownames(indiv_trim_stats[[sample_name]]) <- 
      c("input_read_pairs", "r1_detect_adapter", "r2_detect_adapter", 
        "filtered_pairs", "passed_pairs", "total_input_bp", "total_output_bp")
  } else {
    indiv_trim_stats[[sample_name]] <- sapply(summary_indicies, function(j) {
      extract_cutadapt_stats(log_file_txt[j:(j+16)])})
  }
}

trim_stats <- sapply(indiv_trim_stats, rowSums)
trim_stats_df <- data.frame(t(trim_stats)) %>% rownames_to_column("sample_id")
paged_table(trim_stats_df)
```

The columns in the above table refer to:

- `sample-id`: the sample name and amplicon group
- `input_read_pairs`: the number of read pairs that were processed
- `r1_detect_adapter`: the number of forward reads detected to contain the adapter
- `r2_detect_adapter`: the number of reverse reads detected to contain the adapter
- `filtered_pairs`: the number of reads removed after trimming due to being too short
- `passed_pairs`: the number of reads that passed after trimming
- `total_bp`: the number of base pairs that 
- `total_input_bp`: the number of base pairs that were processed
- `total_output_bp`: the number of base pairs that passed after trimming


-----

# Summary and visualisation

Create visualisation files from the previous job.

```{r}
summary_dir <- file.path(results_dir, "summary")
mkdir(summary_dir)

# Setup targets
targets <- data.frame(FileName=extract_targets(trim, "output"),
                      SampleName=extract_targets(trim, "SampleName"),
                      Factor=extract_targets(trim, "Factor"),
                      metadata_path=metadata_filename,
                      sample_id=extract_targets(trim, "sample_id"),
                      amplicon_group=extract_targets(trim, "amplicon_group"))
targets <- targets %>% 
  mutate(output=file.path(summary_dir, paste0(SampleName, ".qzv")),
         output_dir=summary_dir)
summary_targets <- here("tmp/summary_targets.txt")
write_targets(targets, file=summary_targets)

# Setup workflow
summary <- loadWorkflow(targets=summary_targets, wf_file="qiime_summary.cwl", 
    input_file="qiime_summary.yml", dir_path=cwl_path)
summary <- set_logs_dir(summary, logs_dir)
summary@yamlinput$summary_wrapper$path <- file.path(cwl_path, "qiime_summary_wrapper.sh")
summary <- renderWF(summary, inputvars=c(FileName="_INPUT_PATH_", output="_OUTPUT_PATH_",
                                         output_dir="_OUTPUT_DIR_", SampleName="_SAMPLE_NAME_"))
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
run_jobs(summary)
```


:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(summary)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(summary_dir)
```

:::

::::



### Per sample FASTQ counts

Count how many reads per sample, amplicon group, and primer combination.

```{r}
# Load per-sample-fastq-counts files
summary_files <- list.files(summary_dir, "*per-sample-fastq-counts.tsv", full.names=TRUE)
suppressMessages(
  summary_list <- lapply(summary_files, read_tsv)
)
names(summary_list) <- str_remove(basename(summary_files), 
                                  "_per-sample-fastq-counts.tsv")

# Combine into a single data frame
summary_df <- lapply(names(summary_list), function(x) {
  summary_list[[x]] %>% mutate(library=x)
}) %>% do.call(rbind, .) %>%
  left_join(metadata_df %>% select(`sample-id`, group), by=c("sample ID"="sample-id")) %>% select(library, group, everything())
```



::::: {.panelset}

::: {.panel}

#### Counts per sample {.panel-name}

```{r}
summary_df %>% 
  group_by(`library`) %>% summarise(n=sum(`forward sequence count`)) %>%
  paged_table
```

:::

::: {.panel}

#### Counts per group {.panel-name}

```{r}
summary_df %>% 
  group_by(group) %>% 
  summarise(n=sum(`forward sequence count`)) %>%
  paged_table
```

:::

::: {.panel}

#### Counts per primer combination {.panel-name}

```{r}
summary_df %>% 
  group_by(`sample ID`, group) %>% summarise(n=sum(`forward sequence count`)) %>%
  paged_table
```

:::

::: {.panel}

#### Counts per sample & group & primer combination {.panel-name}


```{r}
summary_df %>% 
  paged_table
```

:::

::::


### Quality plots

Output plots to visualise read quality scores.

```{r}
wrangle_seven_number_summary <- function(df) {
  cols <- df[,1,drop=TRUE]
  dat <- data.frame(t(df[,-1]))
  colnames(dat) <- cols
  dat <- dat %>% rownames_to_column("sequence_base") %>%
    mutate(sequence_base=as.numeric(sequence_base))
  return(dat)
}

sequence_quality_plot <- function(dat, title) {
  y_min <- min(0, dat[["9%"]])
  y_max <- max(40, dat[["91%"]])
  ggplot(dat, aes(x=sequence_base, ymin=`9%`, lower=`25%`, middle=`50%`, 
                               upper=`75%`, ymax=`91%`, group=sequence_base)) +
    geom_boxplot(stat="identity") +
    scale_y_continuous(limits=c(y_min, y_max)) +
    theme_bw() +
    labs(title=title,
         x="Sequence Base",
         y="Quality Score") +
    theme(plot.title = element_text(size=12))
}
```

Load TSV file containing quality scores.

```{r}
# Load per-sample-fastq-counts files
forward_files <- list.files(summary_dir, "*forward-seven-number-summaries.tsv", full.names=TRUE)
reverse_files <- list.files(summary_dir, "*reverse-seven-number-summaries.tsv", full.names=TRUE)
suppressWarnings(suppressMessages(
  forward_list <- lapply(forward_files, read_tsv)
))
suppressWarnings(suppressMessages(
  reverse_list <- lapply(reverse_files, read_tsv)
))
names(forward_list) <- str_remove(basename(forward_files), 
                                  "_forward-seven-number-summaries.tsv")
names(reverse_list) <- str_remove(basename(reverse_files), 
                                  "_reverse-seven-number-summaries.tsv")

forward_dat <- lapply(forward_list, wrangle_seven_number_summary)
reverse_dat <- lapply(reverse_list, wrangle_seven_number_summary)

# Get amplicon group for each file
group <- forward_files %>% 
  basename() %>% 
  str_remove("_forward-seven-number-summaries.tsv$") %>% 
  str_split("_") %>% 
  sapply(function(x) x[length(x)])
```

Plot median quality score for each sample across sequence bases:

```{r}
# Max count
max_count <- 10000
for (amplicon_group in amplicon_groups) {
  file_indices <- which(group == amplicon_group)
  group_sample_names <- names(forward_dat[file_indices])
  forward_df <- do.call(rbind, lapply(group_sample_names, function(x) {
    forward_dat[[x]] %>% select(sequence_base, count, `50%`) %>% mutate(sample_id=x)
    }))
  reverse_df <- do.call(rbind, lapply(group_sample_names, function(x) {
    reverse_dat[[x]] %>% select(sequence_base, count, `50%`) %>% mutate(sample_id=x)
    }))
  g_forward <- ggplot(forward_df, 
                      aes(x=sequence_base, y=`50%`, color=sample_id, alpha=count/max_count)) +
    geom_line() +
    scale_y_continuous(limits=c(0, 40)) +
    scale_alpha_continuous(limits=c(0, 1)) + 
    theme_bw() +
    labs(title=sprintf("Forward reads - Amplicon group: %s", amplicon_group),
         x="Sequence Base",
         y="Median Quality Score") +
    theme(plot.title = element_text(size=12))
  g_reverse <- ggplot(reverse_df, 
                      aes(x=sequence_base, y=`50%`, color=sample_id, alpha=count/max_count)) +
    geom_line() +
    scale_y_continuous(limits=c(0, 40)) +
    scale_alpha_continuous(limits=c(0, 1)) + 
    theme_bw() +
    labs(title=sprintf("Reverse reads - Amplicon group: %s", amplicon_group),
         x="Sequence Base",
         y="Median Quality Score") +
    theme(plot.title = element_text(size=12))
  print(g_forward)
  print(g_reverse)
}
```

If you want to view an individual sample's sequence base quality boxplot, you can view the sample's qzv file located in:
**`r summary_dir`**  
using [QIIME 2 View](https://view.qiime2.org/), or run the `sequencing_quality_boxplots.Rmd` notebook to view all sample boxplots.

-----

# Save output

Save output so it can be loaded in subsequent notebooks.

```{r}
mkdir(here("cache"))
save.image(here("cache/01.RData"))
```

-----

# Session Info

```{r}
if (nzchar(system.file(package="devtools"))) {
  devtools::session_info()
} else {
  sessionInfo()
}
```

