---
title: "Metabarcoding Quantification"
description: |
  A QIIME2-based metabarcoding workflow. Part 2.
author:
  - name: Your Name Here
    url: https://example.com/
    affiliation: Example Organisation
    affiliation_url: https://example.com/
    orcid_id: 0000-0000-0000-0000
  - name: Jessica Chung
    url: https://github.com/jessicachung
    affiliation: Melbourne Bioinformatics, PEARG
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 4
    df_print: paged
    code_folding: true
    highlight: haddock
    highlight_downlit: false
    css: "custom.css"
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=TRUE, message=TRUE, error=TRUE, echo=TRUE, results="hold")
knitr::opts_knit$set(root.dir = "..")
options(digits=4)
options(width=120)
```


This notebook performs quantification for your metabarcoding analysis. This notebook depends on the outputs of the previous notebook, `01_preprocessing.Rmd` where we preprocessed the sequencing data. In this notebook, first, we'll create new QIIME artifact files for each amplicon group, so each file contains all samples corresponding to that group. We'll then use DADA2 to denoise sequence data and generate feature tables and feature data. Lastly, we'll run [LULU](https://github.com/tobiasgf/lulu) to perform distribution-based post-clustering curation of the ASV sequences.


# Load

```{r message=FALSE, warning=FALSE}
suppressPackageStartupMessages({
  library(systemPipeR)
  library(tidyverse)
  library(here)
  library(patchwork)
  library(rmarkdown)
  library(fs)
  library(qiime2R)
  library(lulu)
})

xaringanExtra::use_panelset()

# Load workspace from the previous notebook
load(here::here("cache/01.RData"))
```

```{r}
# Append qiime2 path to PATH
current_path_env <- Sys.getenv("PATH")
Sys.setenv(PATH=paste0("/mnt/galaxy/gvl/software/shared_envs/qiime2/bin:",
                       current_path_env))
Sys.setenv(PYTHONNOUSERSITE="/mnt/galaxy/gvl/software/shared_envs/qiime2/lib/python*/site-packages/")
```

```{r}
use_slurm <- TRUE
slurm_threads <- 8
slurm_resources <- list(partition="main", ntasks=1, ncpus=slurm_threads, memory=4096)
slurm_n_jobs <- 10
```


# Combine into amplicon groups

### Extract step

Create a directory for each amplicon group.

```{r}
# Create output directories (separate directory for each amplicon group)
combined_dir <- file.path(results_dir, "combined")
mkdir(combined_dir)

for (a in amplicon_groups) {
  mkdir(file.path(combined_dir, a))
}
```

```{r}
# Setup targets
targets <- data.frame(FileName=extract_targets(trim, "output"),
                      SampleName=extract_targets(trim, "SampleName"),
                      Factor=extract_targets(trim, "Factor"),
                      amplicon_group=extract_targets(trim, "amplicon_group"))


targets <- targets %>% 
  mutate(output_r1=file.path(combined_dir, amplicon_group, 
                             paste0(Factor, "_X_L001_R1_001.fastq.gz")),
         output_r2=file.path(combined_dir, amplicon_group, 
                             paste0(Factor, "_X_L001_R2_001.fastq.gz")))

combine_1_targets <- here("tmp/combine_1_targets.txt")
write_targets(targets, file=combine_1_targets)
```



::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
combine_1 <- loadWorkflow(targets=combine_1_targets, wf_file="qiime_combine_extract.cwl", 
    input_file="qiime_combine_extract.yml", dir_path=cwl_path)
combine_1 <- set_logs_dir(combine_1, logs_dir)
combine_1@yamlinput$extract_wrapper$path <- file.path(cwl_path, "qiime_combine_extract_wrapper.sh")
combine_1@yamlinput$output_dir$path <- combined_dir
combine_1 <- renderWF(combine_1, inputvars=c(FileName="_INPUT_PATH_",
                                             output_r1="_OUTPUT_R1_PATH_",
                                             output_r2="_OUTPUT_R2_PATH_",
                                             amplicon_group="_AMPLICON_GROUP_",
                                             Factor="_SAMPLE_NAME_"))
run_jobs(combine_1)
```

:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(combine_1)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(combined_dir)
```

:::

::::


### Import step

Import files into a single qiime artifact file for each amplicon group.

```{r}
# Setup targets
targets <- data.frame(FileName=dirname(extract_targets(combine_1, "output_r1")),
                      SampleName=extract_targets(combine_1, "amplicon_group"),
                      amplicon_group=extract_targets(combine_1, "amplicon_group"))
targets <- unique(targets)

targets <- targets %>% 
  mutate(output=file.path(combined_dir, paste0(amplicon_group, ".qza")))

combine_2_targets <- here("tmp/combine_2_targets.txt")
write_targets(targets, file=combine_2_targets)
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
combine_2 <- loadWorkflow(targets=combine_2_targets, wf_file="qiime_combine_import.cwl", 
    input_file="qiime_combine_import.yml", dir_path=cwl_path)
combine_2 <- set_logs_dir(combine_2, logs_dir)
combine_2 <- renderWF(combine_2, inputvars=c(FileName="_INPUT_PATH_",
                                             output="_OUTPUT_PATH_"))
run_jobs(combine_2)
```

:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(combine_2)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(combined_dir)
```

:::

::::


-----

# DADA2

First, define the lengths that amplicon groups will be truncated to.

```{r}
# TODO: Custom truncation lengths from file? Don't know how this should be handled
truncation_lengths <- data.frame(
  SampleName = c("short", "left", "right", "long"),
  forward_trunc_length = c(140, 200, 140, 200),
  reverse_trunc_length = c(140, 200, 200, 200)
)
paged_table(truncation_lengths)
```

Now we'll run DADA2 to quantify ASVs.

```{r}
dada2_dir <- file.path(results_dir, "dada2")
mkdir(dada2_dir)

# Setup targets
targets <- data.frame(FileName=extract_targets(combine_2, "output"),
                      SampleName=extract_targets(combine_2, "SampleName"),
                      amplicon_group=extract_targets(combine_2, "amplicon_group"))

# Join with truncation length parameters and check no missing values
targets <- full_join(targets, truncation_lengths, by="SampleName")
stopifnot(all(! is.na(targets)))


targets <- targets %>% 
  mutate(stats_output=file.path(dada2_dir, paste0(amplicon_group, "_denoising_stats.qza")),
         seq_output=file.path(dada2_dir, paste0(amplicon_group, "_representative_sequences.qza")),
         table_output=file.path(dada2_dir, paste0(amplicon_group, "_table.qza"))
  )

# targets <- targets %>% filter(SampleName == "short")
targets <- targets %>% filter(SampleName %in% c("short", "right"))

dada2_targets <- here("tmp/dada2_targets.txt")
write_targets(targets, file=dada2_targets)
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
dada2 <- loadWorkflow(targets=dada2_targets, wf_file="qiime_dada2.cwl", 
    input_file="qiime_dada2.yml", dir_path=cwl_path)
dada2 <- set_logs_dir(dada2, logs_dir)
dada2 <- renderWF(dada2, inputvars=c(FileName="_INPUT_PATH_",
                                     forward_trunc_length="_FORWARD_TRUNC_",
                                     reverse_trunc_length="_REVERSE_TRUNC_",
                                     stats_output="_STATS_PATH_",
                                     seq_output="_SEQ_PATH_",
                                     table_output="_TABLE_PATH_"))
run_jobs(dada2)
```


:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(dada2)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(dada2_dir)
```

:::

::::

-----

# DADA2 visualisation

```{r}
# Setup targets
targets <- data.frame(FileName=extract_targets(dada2, "seq_output"),
                      SampleName=extract_targets(dada2, "SampleName"),
                      amplicon_group=extract_targets(dada2, "amplicon_group"),
                      stats_qza=extract_targets(dada2, "stats_output"))

targets <- targets %>% 
  mutate(stats_qzv=
           file.path(dada2_dir, paste0(amplicon_group, "_denoising_stats.qzv")),
  )

dada2_vis_targets <- here("tmp/dada2_vis_targets.txt")

write_targets(targets, file=dada2_vis_targets)
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
dada2_vis <- loadWorkflow(targets=dada2_vis_targets, wf_file="qiime_dada2_vis.cwl", 
                          input_file="qiime_dada2_vis.yml", dir_path=cwl_path)
dada2_vis <- set_logs_dir(dada2_vis, logs_dir)
dada2_vis <- renderWF(dada2_vis, inputvars=c(stats_qza="_INPUT_PATH_",
                                             stats_qzv="_OUTPUT_QZV_"))
run_jobs(dada2_vis)
```


:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(dada2_vis)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(dada2_dir)
```

:::

::::



# View DADA2 stats

```{r}
paged_table(read_qza(here("results/dada2/short_denoising_stats.qza"))$data)
```

```{r}
paged_table(read_qza(here("results/dada2/right_denoising_stats.qza"))$data)
```


-----

# DADA2 filtering

Remove low frequency ASVs from DADA2 tables. First filter table with `filter-features-conditionally`, then filter matching sequences with `filter-seqs`.

```{r}
# Setup targets
targets <- data.frame(FileName=extract_targets(dada2, "table_output"),
                      SampleName=extract_targets(dada2, "SampleName"),
                      amplicon_group=extract_targets(dada2, "amplicon_group"),
                      table_input=extract_targets(dada2, "table_output"),
                      seq_input=extract_targets(dada2, "seq_output"))

targets <- targets %>% 
  mutate(filtered_table_output=file.path(dada2_dir, paste0(amplicon_group, "_table_filtered.qza")),
         filtered_seq_output=file.path(dada2_dir, paste0(amplicon_group, "_representative_sequences_filtered.qza"))
  )

dada2_filtered_targets <- here("tmp/dada2_filtered_targets.txt")
write_targets(targets, file=dada2_filtered_targets)
```

```{r}
# Set p_abundance and p_prevalence for qiime feature-table filter-features-conditionally.
# (keep these parameters as strings so they don't get converted to scientific notation further down)

# The minimum relative abundance for a feature to be retained
p_abundance <- "0.00001"
# p_abundance <- "0.001" # Lower threshold for testing purposes

# The minimum portion of samples that a feature must have a relative abundance of at least 
# `abundance` to be retained
p_prevalence <- "0.01"
```



::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
dada2_filtering <- loadWorkflow(targets=dada2_filtered_targets, wf_file="qiime_dada2_filtering.cwl", 
                                input_file="qiime_dada2_filtering.yml", dir_path=cwl_path)
dada2_filtering <- set_logs_dir(dada2_filtering, logs_dir)
dada2_filtering@yamlinput$dada2_filtering_wrapper$path <- 
  file.path(cwl_path, "qiime_dada2_filtering_wrapper.sh")
dada2_filtering@yamlinput$p_abundance <- p_abundance
dada2_filtering@yamlinput$p_prevalence <- p_prevalence
dada2_filtering@yamlinput$output_dir$path <- dada2_dir

dada2_filtering <- renderWF(dada2_filtering, inputvars=c(FileName="_TABLE_INPUT_",
                                                         seq_input="_SEQ_INPUT_",
                                                         filtered_table_output="_TABLE_OUTPUT_",
                                                         filtered_seq_output="_SEQ_OUTPUT_"))
run_jobs(dada2_filtering)
```


:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(dada2_filtering)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(dada2_dir)
```

:::

::::


-----

# Check filtering

Check dimensions of the table data pre and post-filtering.

```{r}
before_filtering <- read_qza(file.path(dada2_dir, "short_table.qza"))
after_filtering <- read_qza(file.path(dada2_dir, "short_table_filtered.qza"))
```

```{r}
print("Pre-filtering:")
dim(before_filtering$data)

print("Post-filtering:")
dim(after_filtering$data)
```


-----

# LULU

## Run vsearch

Create a match list file for LULU. This step uses vsearch to match a collection of ASVs against themselves and
outputs a file to be used in the next step.

```{r}
lulu_dir <- file.path(results_dir, "lulu")
mkdir(lulu_dir)

# Setup targets
targets <- data.frame(FileName=extract_targets(dada2_filtering, "filtered_seq_output"),
                      SampleName=extract_targets(dada2_filtering, "SampleName"),
                      amplicon_group=extract_targets(dada2_filtering, "amplicon_group"))

targets <- targets %>% 
  mutate(match_list_path=
           file.path(lulu_dir, paste0(amplicon_group, "_match_list.txt"))
  )

lulu_vsearch_targets <- here("tmp/lulu_vsearch_targets.txt")

write_targets(targets, file=lulu_vsearch_targets)
```


::::: {.panelset}

::: {.panel}

#### Run output {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
lulu_vsearch <- loadWorkflow(targets=lulu_vsearch_targets,
                                           wf_file="lulu_vsearch_prelim.cwl", 
                                           input_file="lulu_vsearch_prelim.yml", dir_path=cwl_path)
lulu_vsearch <- set_logs_dir(lulu_vsearch, logs_dir)
lulu_vsearch@yamlinput$lulu_vsearch_prelim_wrapper$path <- 
  file.path(cwl_path, "lulu_vsearch_prelim_wrapper.sh")
lulu_vsearch@yamlinput$output_dir$path <- lulu_dir
lulu_vsearch <- renderWF(lulu_vsearch, 
                         inputvars=c(FileName="_INPUT_REP_SEQ_",
                                     match_list_path="_OUTPUT_TSV_"))

run_jobs(lulu_vsearch)
```


:::

::: {.panel}

#### Command list {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# Print commands that were run
cmdlist(lulu_vsearch)
```

:::

::: {.panel}

#### Output directory files {.panel-name}

```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(lulu_dir)
```

:::

::::


## Run lulu

Run LULU to merge features together.


```{r}
# TODO: encapsulate this into a function after checking if lulu arguments need to be changed
```

```{r}
short_table <- read_qza(file.path(dada2_dir, "short_table_filtered.qza"))
short_table_df <- data.frame(short_table$data)
short_match_list <- read.table(file.path(lulu_dir, "short_match_list.txt"))

# TODO: Currently using default parameters
z <- capture.output(short_lulu <- lulu(short_table_df, short_match_list))

dim(short_lulu$original_table)
dim(short_lulu$curated_table)
```

```{r}
saveRDS(short_lulu, file=file.path(lulu_dir, "short_lulu.rds"))
```


```{r}
right_table <- read_qza(file.path(dada2_dir, "right_table_filtered.qza"))
right_table_df <- data.frame(right_table$data)
right_match_list <- read.table(file.path(lulu_dir, "right_match_list.txt"))

# TODO: Currently using default parameters
z <- capture.output(right_lulu <- lulu(right_table_df, right_match_list))

dim(right_lulu$original_table)
dim(right_lulu$curated_table)
```

```{r}
saveRDS(right_lulu, file=file.path(lulu_dir, "right_lulu.rds"))
```


```{r, attr.output='style="max-height: 400px; overflow: auto !important;"'}
# List files in output directory
dir_tree(lulu_dir)
```

-----
-----

# Save output

Save output so it can be loaded in subsequent notebooks.

```{r}
save.image(here("results/cache/02.RData"))
```


-----

# Session Info

```{r}
if (nzchar(system.file(package="devtools"))) {
  devtools::session_info()
} else {
  sessionInfo()
}
```

